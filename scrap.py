# scrapbook

SAMPLE_RESPONSE_IF_MAX_RETURN_TOKENS_IS_60 = '''
: Chat GLM 6B is an open-source chat GPT-like model that can run on a single consumer GPU with under 24GB of memory and can be used for summarization and back-and-forth conversations. It is fast, efficient, and has a good memory footprint.
'''

SAMPLE_RESPONSE_IF_MAX_RETURN_TOKENS_IS_2000 = '''
: Chat GLM-6B is an open source model with 10 billion parameters, capable of running on consumer grade hardware and good for summarization tasks. It works with English and Chinese, but causes some unexpected issues while training. Chat GLM-130B is a larger model with 130 billion parameters for more complex tasks.
'''

SAMPLE_RESPONSE_IF_MAX_RETURN_TOKENS_IS_2000_2 = '''
: Chat GLM 6B is a 6.2 billion parameter GPT model designed for Chinese dialogue that runs on consumer hardware and is considered one of the best small chat-style GPT models. It uses multi-task training, bi-directional attention, and potentialy Glu activation functions. There are more models coming soon, with opinions varying greatly on which is best.
'''
